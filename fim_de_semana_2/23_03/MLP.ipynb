{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f9d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "# Code from: https://www.youtube.com/watch?v=0oWnheK-gGk and https://www.youtube.com/watch?v=Z97XGNUUx9o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d09b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, num_inputs=3, num_hidden=[3, 5], num_outputs=2):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        layers = [self.num_inputs] + self.num_hidden + [self.num_outputs]\n",
    "\n",
    "        # Initiate random weights.\n",
    "        weights = []\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            w = np.random.rand(layers[i], layers[i+1])\n",
    "            weights.append(w)\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "        activations = []\n",
    "        for i in range(len(layers)):\n",
    "            a = np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "\n",
    "        self.activations = activations\n",
    "\n",
    "        derivatives = []\n",
    "        for i in range(len(layers) - 1): # Same number of weight matrices.\n",
    "            d = np.zeros((layers[i], layers[i+1]))\n",
    "            derivatives.append(d)\n",
    "\n",
    "        self.derivatives = derivatives\n",
    "\n",
    "\n",
    "    def forward_propagate(self, inputs):\n",
    "        activations = inputs\n",
    "        self.activations[0] = inputs\n",
    "\n",
    "        for i, w in enumerate(self.weights):\n",
    "            # Calculate net inputs.\n",
    "            net_inputs = np.dot(activations, w)\n",
    "            \n",
    "            # Calculate the activateions.\n",
    "            activations = self._sigmoid(net_inputs)\n",
    "            self.activations[i+1] = activations\n",
    "\n",
    "        return activations\n",
    "\n",
    "    def back_propagate(self, error, verbose=False):\n",
    "\n",
    "        for i in reversed(range(len(self.derivatives))):\n",
    "            activations = self.activations[i+1]\n",
    "            delta = error * self._sigmoid_derivative(activations)\n",
    "            delta_reshaped = delta.reshape(delta.shape[0], -1).T\n",
    "            current_activation = self.activations[i]\n",
    "            current_activation_reshaped = current_activation.reshape(current_activation.shape[0], -1)\n",
    "\n",
    "            self.derivatives[i] = np.dot(current_activation_reshaped, delta_reshaped)\n",
    "            error = np.dot(delta, self.weights[i].T)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Derivatives for W{i}: {self.derivatives[i]}\")\n",
    "\n",
    "        return error\n",
    "\n",
    "    def gradient_descent(self, learning_rate):\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # weights = self.weights[i]\n",
    "            # derivatives = self.derivatives[i]\n",
    "            self.weights[i] += self.derivatives[i] * learning_rate\n",
    "\n",
    "    def train(self, inputs, targets, epochs, learning_rate):\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            for input, target in zip(inputs, targets):\n",
    "                # Perform forward prop.\n",
    "                output = self.forward_propagate(input)\n",
    "\n",
    "                # Calculate error.\n",
    "                error = target - output\n",
    "\n",
    "                self.back_propagate(error)#, verbose=True)\n",
    "\n",
    "                # Apply Gradient Descent\n",
    "                self.gradient_descent(learning_rate)\n",
    "\n",
    "                sum_error += self._mse(target, output)\n",
    "                        \n",
    "            # Report error.\n",
    "            print(f\"Error {sum_error/len(inputs)} at epoch {i}\")\n",
    "\n",
    "    def _mse(self, target, output):\n",
    "        return np.average((target - output)**2)\n",
    "\n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return x * (1.0 - x)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6f15069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 0.4592583894673798 at epoch 0\n",
      "Error 0.45777400027178955 at epoch 1\n",
      "Error 0.4562852525265416 at epoch 2\n",
      "Error 0.4547922537147543 at epoch 3\n",
      "Error 0.45329511365883834 at epoch 4\n",
      "Error 0.45179394450277505 at epoch 5\n",
      "Error 0.4502888606922135 at epoch 6\n",
      "Error 0.4487799789523597 at epoch 7\n",
      "Error 0.4472674182636368 at epoch 8\n",
      "Error 0.4457512998350922 at epoch 9\n",
      "Error 0.44423174707553476 at epoch 10\n",
      "Error 0.4427088855623881 at epoch 11\n",
      "Error 0.4411828430082481 at epoch 12\n",
      "Error 0.4396537492251373 at epoch 13\n",
      "Error 0.4381217360864514 at epoch 14\n",
      "Error 0.4365869374865975 at epoch 15\n",
      "Error 0.4350494892983291 at epoch 16\n",
      "Error 0.43350952932778164 at epoch 17\n",
      "Error 0.43196719726722355 at epoch 18\n",
      "Error 0.43042263464553454 at epoch 19\n",
      "Error 0.42887598477643435 at epoch 20\n",
      "Error 0.4273273927044812 at epoch 21\n",
      "Error 0.4257770051488714 at epoch 22\n",
      "Error 0.42422497044506935 at epoch 23\n",
      "Error 0.422671438484307 at epoch 24\n",
      "Error 0.4211165606509892 at epoch 25\n",
      "Error 0.41956048975805554 at epoch 26\n",
      "Error 0.4180033799803415 at epoch 27\n",
      "Error 0.41644538678599774 at epoch 28\n",
      "Error 0.4148866668660212 at epoch 29\n",
      "[0.72839269]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Create an MLP.\n",
    "mlp = MLP(2, [5], 1)\n",
    "\n",
    "# Create a dataset to train a network for the sum operation.\n",
    "#inputs = np.array([[random()/2 for _ in range(2)] for _ in range(1000)])\n",
    "#targets = np.array([[i[0] + i[1]] for i in inputs])\n",
    "\n",
    "inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "targets = np.array([0,0,0,1])\n",
    "\n",
    "# Train our model.\n",
    "mlp.train(inputs, targets, 30, 0.01)\n",
    "result = mlp.forward_propagate(np.array([1,1]))\n",
    "print(result)\n",
    "print(int(np.round(result)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
