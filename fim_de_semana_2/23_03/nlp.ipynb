{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "latin-disability",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "We can understand embeddings as a representation.\n",
    "When working on Machine/Deep Learning models, we need to represent information in a numerical way.\n",
    "This is not a problem when we can extract this type of features directly from our problem, such as the house price problem where we use the *number* of rooms, the slot *area*, etc.\n",
    "However, when working with natural language, we have to convert letters, words, sentences, or entire documents into numerical information.\n",
    "In this notebook, we will see some existing embedding methods for representing natural language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-anime",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-springfield",
   "metadata": {},
   "source": [
    "This is the most basic structure we can use to represent text.\n",
    "The basic idea is to represent a sentence by a vector where each position represents a word and the value in it represents the number of occurrences in the sentence.\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "understood-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences.\n",
    "sentences = [\n",
    "    'I love embeddings',\n",
    "    'I do not like embeddings',\n",
    "    'Love is like trash',\n",
    "    'I am like you and I hate'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-belief",
   "metadata": {},
   "source": [
    "In order to use it properly, we first preprocess the text.\n",
    "In our example, I will just turn all text into lower case, but it can vary, like removing stop-words, among other techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dental-metallic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love embeddings',\n",
       " 'i do not like embeddings',\n",
       " 'love is like trash',\n",
       " 'i am like you and i hate']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess.\n",
    "lower_sentences = [s.lower() for s in sentences]\n",
    "lower_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-company",
   "metadata": {},
   "source": [
    "Then, we will create a set with all unique words from our sentences, this will be our **vocabulary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "atlantic-insulin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do',\n",
       " 'love',\n",
       " 'am',\n",
       " 'i',\n",
       " 'you',\n",
       " 'hate',\n",
       " 'embeddings',\n",
       " 'like',\n",
       " 'and',\n",
       " 'trash',\n",
       " 'is',\n",
       " 'not']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = list()\n",
    "\n",
    "for sentence in lower_sentences:\n",
    "    for word in sentence.split(): # Break sentence into individual words.\n",
    "        word_list.append(word)\n",
    "        \n",
    "word_list = list(set(word_list)) # Turn into a set to remove duplicates. Then back to a list.\n",
    "word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-momentum",
   "metadata": {},
   "source": [
    "Now, our representation will be based on this vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broke-render",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: i love embeddings\n",
      "BOW Representation: [0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Sentence: i do not like embeddings\n",
      "BOW Representation: [1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1]\n",
      "Sentence: love is like trash\n",
      "BOW Representation: [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
      "Sentence: i am like you and i hate\n",
      "BOW Representation: [0, 0, 1, 2, 1, 1, 0, 1, 1, 0, 0, 0]\n",
      "Word List: ['do', 'love', 'am', 'i', 'you', 'hate', 'embeddings', 'like', 'and', 'trash', 'is', 'not']\n"
     ]
    }
   ],
   "source": [
    "# Representing our sentences.\n",
    "bow_sentences = []\n",
    "\n",
    "for sentence in lower_sentences:\n",
    "    bow_sent = [0 for i in word_list]\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        index = word_list.index(word)\n",
    "        bow_sent[index] = bow_sent[index] + 1\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"BOW Representation:\", bow_sent)\n",
    "\n",
    "    bow_sentences.append(bow_sent)\n",
    "print(\"Word List:\", word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-belle",
   "metadata": {},
   "source": [
    "##### Drawbacks\n",
    "\n",
    "Although simple and easy to use, bag-of-words are not useful when we want to capture the semantic meaning of sentences or simply word order.\n",
    "For instance, imagine that we trained a Sentiment Analysis model that classifies sentences as *Positive* and *Negative* regarding the sentiment they express.\n",
    "If we use Bag-of-Words to train our model, the word 'love' and 'like' can end up having a strong weight towards the positive sentiment, while *hate* and *dislike* being considered negative.\n",
    "Considering our small set of sentences, the trained model would be misled by a sentence such as \"*I do not like embeddings*\", as it has the *like* word but, it is negative.\n",
    "Something similar can happen with the \"*love is like trash*\" sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-finding",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-sheet",
   "metadata": {},
   "source": [
    "**TF-IDF** stands for *Term Frequency - Inverse Document Frequency* and represents texts using the frequency of terms regarding the corpus used.\n",
    "With this representation, we are providing a significance score to terms according to their importance in the corpus.\n",
    "For example, consider the word \"*the*\". It is potentially in every English text possible and occurs multiple times.\n",
    "Therefore, it does not carry too much information about the text it is in, *i.e.*, it does not help to differentiate it from the rest of the texts.\n",
    "On the other hand, the word \"*embedding*\" carries a lot of information in it, as it is not common and refers to specific scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-pathology",
   "metadata": {},
   "source": [
    "Consider the following example using our sentences set used in the bag-of-words example.\n",
    "Here, instead of considering a set of sentences, we will say that each sentence is a single document.\n",
    "The change is only in the term we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "domestic-portable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: i love embeddings\n",
      "BOW Representation: [0.0, 0.23104906018664842, 0.0, 0.09589402415059362, 0.0, 0.0, 0.23104906018664842, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Sentence: i do not like embeddings\n",
      "BOW Representation: [0.2772588722239781, 0.0, 0.0, 0.05753641449035617, 0.0, 0.0, 0.13862943611198905, 0.05753641449035617, 0.0, 0.0, 0.0, 0.2772588722239781]\n",
      "Sentence: love is like trash\n",
      "BOW Representation: [0.0, 0.17328679513998632, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07192051811294521, 0.0, 0.34657359027997264, 0.34657359027997264, 0.0]\n",
      "Sentence: i am like you and i hate\n",
      "BOW Representation: [0.0, 0.0, 0.19804205158855578, 0.08219487784336595, 0.19804205158855578, 0.19804205158855578, 0.0, 0.04109743892168297, 0.19804205158855578, 0.0, 0.0, 0.0]\n",
      "['do', 'love', 'am', 'i', 'you', 'hate', 'embeddings', 'like', 'and', 'trash', 'is', 'not']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tf_idf_sentences = list()\n",
    "n_docs = len(lower_sentences)\n",
    "\n",
    "# First, let's calculate the document frequency of the terms.\n",
    "df = [0 for i in word_list]\n",
    "\n",
    "for s in lower_sentences:\n",
    "    # Given a document.\n",
    "    for t in word_list:\n",
    "        # For each term, check if it occurs in the document.\n",
    "        for w in s.split():\n",
    "            # Run through the document words.\n",
    "            if t == w:\n",
    "                # If the word is in the document.\n",
    "                ind = word_list.index(w)\n",
    "                df[ind] = df[ind] + 1 # Update the counter in the term index.\n",
    "                break\n",
    "    \n",
    "# Then, we can calculate the tf (term frequency) of each term and obtain the final result.\n",
    "for s in lower_sentences:\n",
    "    tf_idf_sent = [0 for i in word_list]\n",
    "    words = s.split()\n",
    "    n_words = len(words)\n",
    "\n",
    "    for t in word_list:\n",
    "        # For each term in our vocabulary.\n",
    "        tf = 0\n",
    "\n",
    "        for w in words:\n",
    "            # Find the term among the words in the document.\n",
    "            if t == w:\n",
    "                tf = tf + 1 # Add to the counter.\n",
    "\n",
    "        tf = tf/n_words # Calculate term frequency over document number of words.\n",
    "        ind = word_list.index(t)\n",
    "        \"\"\"\n",
    "            Calculate the TF-IDF by multiplying the term frequency by the document frequency of the term,\n",
    "            i.e., the number of documents in the corpus divided by the number of documents containing the term.\n",
    "        \"\"\"\n",
    "        tf_idf_sent[ind] = tf * math.log(n_docs/df[ind])\n",
    "    \n",
    "    print(\"Sentence:\", s)\n",
    "    print(\"BOW Representation:\", tf_idf_sent)\n",
    "    tf_idf_sentences.append(tf_idf_sent)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-chocolate",
   "metadata": {},
   "source": [
    "As we can see, common terms, such as \"*I*\", obtain lower values compared to less common ones, such as \"*trash*\". The obtained values will serve as a weight for learning methods to consider when classifying such texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-michigan",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-bracelet",
   "metadata": {},
   "source": [
    "Recently, the use of embeddings changed drastically by the use of neural networks to create such representation. One of the first initiatives of using neural networks was the Word2Vec. The main idea was to find a way to have a representation that encapsulates the context in which words are used in a text. To do so, [in their paper](https://arxiv.org/pdf/1301.3781.pdf%C3%AC%E2%80%94%20%C3%AC%E2%80%9E%C5%93) Mikolov et al. describe CBOW and Skip-Gram models as simple neural networks that try to use the surrounding words of a target word to generate a representation.\n",
    "For instance, given the following sentence \"*the cat on the mat*\" and a window with size 4, we could train the \"on\" representation as:\n",
    "\n",
    "- First, CBOW uses the surrounding words to generate an initial word representation;\n",
    "    - Input: \"the\", \"cat\", \"the\", \"mat\"\n",
    "    - Output: \"on\" representation\n",
    "- Then, Skip-gram uses the resulting representation as input to generate the surrounding words.\n",
    "    - Input: \"on\" representation\n",
    "    - Output: \"the\", \"cat\", \"the\", \"mat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-saint",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='https://www.researchgate.net/profile/Wang-Ling-16/publication/281812760/figure/fig1/AS:613966665486361@1523392468791/Illustration-of-the-Skip-gram-and-Continuous-Bag-of-Word-CBOW-models.png' width=600/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-adjustment",
   "metadata": {},
   "source": [
    "The main property of this representation is that it creates vectors for words that contain the semantic meaning of the word based on the context. This way, all words can be represented in the same dimensionality space (all words have the same vector size), which allows us to identify similarities between words by their distance. It is even possible to perform mathematical operations, such as:\n",
    "\n",
    "V(king) - V(man) + V(woman) = V(queen)\n",
    "\n",
    "In this example, we modify the *king* vector by subtracting *man* and adding *woman*. As result, we obtain the *queen* vector. Actually, the result is not exactly the queen vector, but queen will be the closest one to the resulting vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-radius",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='https://1.bp.blogspot.com/-VhOFQH--Izo/XVfXQ2xWOUI/AAAAAAAANw8/n6VCsT6z_OMWdbmx3O2snLeJOJiJcT4LwCLcBGAs/s1600/w2v_001.png' width=600/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-master",
   "metadata": {},
   "source": [
    "Here, we will use an existing library to perform Word2Vec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "moving-alexandria",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-34beb2cca46e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Python program to generate word vectors using Word2Vec\n",
    "\n",
    "# importing all necessary modules\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Reads ‘alice.txt’ file\n",
    "sample = open(\"supporting_texts/alice.txt\", \"r\")\n",
    "s = sample.read()\n",
    "\n",
    "# Replaces escape character with space\n",
    "f = s.replace(\"\\n\", \" \")\n",
    "\n",
    "data = []\n",
    "\n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(f):\n",
    "    temp = []\n",
    "\n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "\n",
    "    data.append(temp)\n",
    "\n",
    "# Create CBOW model\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,\n",
    "                        vector_size = 50, window = 5, seed=42)\n",
    "\n",
    "# Create Skip Gram model\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 50,\n",
    "                                            window = 5, sg = 1, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dutch-camel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13175842  0.07318828  0.23259278  0.11238079  0.08772991 -0.1094278\n",
      "  0.01447148  0.21727201 -0.3275798   0.0608438   0.08709006  0.34696513\n",
      " -0.13289163  0.44468698  0.17632636 -0.0565002   0.06107061 -0.4055396\n",
      " -0.4012241  -0.05726532 -0.39340606 -0.32697043 -0.08423092  0.19419779\n",
      "  0.6429302  -0.12702861 -0.08809735 -0.16599075 -0.34790352 -0.33239952\n",
      " -0.09282667  0.05073008 -0.26013282  0.25128275 -0.42947346  0.28955492\n",
      " -0.1756014  -0.7376331  -0.00924839  0.25418404  0.03287344  0.23274258\n",
      "  0.5267652  -0.10837778  0.0453444  -0.02558453  0.3589811   0.2514291\n",
      "  0.18430313 -0.11240748]\n",
      "[ 0.1984804   0.16906266  0.2556089   0.10391572  0.02816753 -0.05490195\n",
      "  0.01585247  0.22863606 -0.34660432  0.09359525  0.19915769  0.41965812\n",
      " -0.19819796  0.4896722   0.3313081  -0.04547765  0.17936224 -0.4153227\n",
      " -0.4469249  -0.11902079 -0.42439133 -0.43051377 -0.18797913  0.2214064\n",
      "  0.75050586 -0.21559669 -0.09494667 -0.25087526 -0.43159768 -0.4114146\n",
      " -0.14018968  0.13352662 -0.29370087  0.2980812  -0.33700067  0.37835073\n",
      " -0.3493252  -0.8687064   0.0016202   0.3095833   0.07914691  0.21738997\n",
      "  0.52968633 -0.05271733  0.00409123 -0.05056337  0.40942436  0.2675017\n",
      "  0.13334176 -0.0228832 ]\n",
      "[('thought', 0.9838846325874329), ('king', 0.9809858798980713), ('he', 0.9788033962249756), ('said', 0.9785441756248474), ('hatter', 0.9763650298118591), ('herself', 0.9763294458389282), ('!', 0.9735053777694702), ('very', 0.9728586077690125), ('tone', 0.9717801213264465), ('much', 0.9710057973861694)]\n",
      "[('hatter', 0.9952008128166199), ('gryphon', 0.9892651438713074), ('he', 0.9887219667434692), ('tone', 0.9879248738288879), ('mouse', 0.9861737489700317), ('caterpillar', 0.9842413663864136), ('duchess', 0.9839333891868591), ('replied', 0.9834162592887878), ('queen', 0.9828681349754333), ('!', 0.982623815536499)]\n"
     ]
    }
   ],
   "source": [
    "print(model2.wv['king'])\n",
    "print(model2.wv['alice'])\n",
    "print(model2.wv.most_similar('alice', topn=10))\n",
    "print(model2.wv.most_similar('king', topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceramic-conditioning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king', 0.9757580161094666), ('said', 0.9702053666114807), ('alice', 0.9692055583000183), ('hatter', 0.9616694450378418), ('”', 0.9567456841468811), ('gryphon', 0.9541082978248596), ('thought', 0.9510892033576965), ('he', 0.9500472545623779), ('mock', 0.9488303661346436), ('herself', 0.9461054801940918)]\n"
     ]
    }
   ],
   "source": [
    "vking = model2.wv['king']\n",
    "vman = model2.wv['man']\n",
    "vwoman = model2.wv['woman']\n",
    "vqueen = vking - vman + vwoman\n",
    "print(model2.wv.most_similar(vqueen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-miniature",
   "metadata": {},
   "source": [
    "Since we are using a small dataset here, the idea of subtracting *man* and adding *woman* from *king* did not work as expected. However, *queen* is in the list of most similar to our resulting vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-setting",
   "metadata": {},
   "source": [
    "### Sent2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-impression",
   "metadata": {},
   "source": [
    "Inspired by Word2Vec, [Pagliardini et al.](https://aclanthology.org/N18-1049.pdf) use a similar method to generate representations to entire sentences and documents.\n",
    "In this case, instead of just words, we will also have sentences and paragraphs as context.\n",
    "They generate sentence representations by averaging the words in the input sentence.\n",
    "Thus, they train a model to predict the next word.\n",
    "This method (as well as Word2Vec) is considered to be unsupervised, since we are not using an annotation of data to train the model.\n",
    "Instead, we use the text itself, so the model learns by input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-aaron",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='https://miro.medium.com/max/1400/1*RyWXrpAxzzO_zzZgtMN1mQ.png' width=600>\n",
    "<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-queue",
   "metadata": {},
   "source": [
    "In order to use a sent2vec model as described in the paper, we can follow the instructions from the authors in the paper [repository](https://github.com/epfml/sent2vec).\n",
    "Since it relies on downloading heavy models, I will skip it in this notebook.\n",
    "But I encourage you to test it by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-seventh",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-zimbabwe",
   "metadata": {},
   "source": [
    "[BERT](https://arxiv.org/pdf/1810.04805.pdf) stands for **B**idirectional **E**ncoder **R**epresentations from **T**ransformers.\n",
    "As its name says, it is based on a previous model called [Transformer](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).\n",
    "A Transformer consists of an Encoder and a Decoder, and it is commonly used for Machine Translation.\n",
    "In general terms, the Encoder creates a representation to the input and the Decoder processes it to convert in the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-capacity",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='https://production-media.paperswithcode.com/methods/new_ModalNet-21.jpg' width=300>\n",
    "<center/>\n",
    "Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-plate",
   "metadata": {},
   "source": [
    "BERT exploits the encoder part of Transformer to generate rich representations to the input and places a classifier head at the end to be trained as a standard model.\n",
    "The encoder uses self-attention layers to process the information in a way that it can focus on what is more important for the specific terms regarding the whole input.\n",
    "It makes each representation unique according to the context.\n",
    "For example, using Word2Vec, we generate a unique representation to a word.\n",
    "However, we know that words can have multiple meanings.\n",
    "For the Word2Vec representation, all these meanings are combined in a single vector.\n",
    "We say, then, that Word2Vec is **context-independent**.\n",
    "On the other hand, BERT is **context-dependent**, *i.e.*, it generates different representations to different meanings of the same word depending on the context it is in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-beijing",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src='https://www.researchgate.net/publication/349546860/figure/fig2/AS:994573320994818@1614136166736/The-Transformer-based-BERT-base-architecture-with-twelve-encoder-blocks.ppm' width=600>\n",
    "<center/>\n",
    "BERT architecture example for a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-range",
   "metadata": {},
   "source": [
    "Consider that we have the following two sentences:\n",
    "1. We went to the *river* **bank**.\n",
    "2. I need to go to the **bank** to *make a deposit*.\n",
    "\n",
    "In bold, **bank** is the word with different meanings for each of the two sentences.\n",
    "In italic, is the context that allows us to understand the meaning of bank.\n",
    "While word2vec will generate a single vector of bank for both sentences, BERT generates different ones.\n",
    "The reason for this is that Word2Vec needs only a single word as input to generate the representation, while BERT always needs to have the entire sentence as input to generate the representation of a single word.\n",
    "\n",
    "To improve your understanding on this subject, please, read this following [blog post](https://medium.com/swlh/differences-between-word2vec-and-bert-c08a3326b5d1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-gentleman",
   "metadata": {},
   "source": [
    "In order to use BERT, we have multiple existing python libraries that allows us to load pre-trained models and use them for classification tasks, but also for embedding generation.\n",
    "Since it involves the use of Machine Learning frameworks, such as PyTorch, TensorFlow, and the Transformers library, I will cover them in the next notebook, which will be focused on frameworks for Machine Learning.\n",
    "Now, you can follow the steps provided by the [TensorFlow page](https://www.tensorflow.org/hub/tutorials/bert_experts) to use existing pre-trained BERT models on sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
